{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# AMAT 502: Modern Computing for Mathematicians\n",
    "## Lecture 23 - Decision Trees and Entropy\n",
    "### University at Albany SUNY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topics for Today\n",
    "\n",
    "- Decision Trees\n",
    "- Entropy\n",
    "- Entropy of a partition and Information Gain\n",
    "- The ID3 Algorithm\n",
    "- Example on Hiring Decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More References\n",
    "\n",
    "- A group of researchers at Colorado State University has made precise the number of support vectors in a generic configuration. You can [read their paper](https://arxiv.org/abs/2011.00617) that was posted to the ArXiv on November 1st, 2020: \n",
    "\n",
    "- The above paper makes use of Radon's theorem, which is part of any good education on convex geometry and linear programming. You should consider taking a course in Linear Programming that follows either\n",
    "    - Bertsimas and Tsitsiklis's [*Introduction to Linear Optimization*](http://www.athenasc.com/linoptbook.html)\n",
    "    - Matousek and Gärtner's [*Understanding and Using Linear Programming*](https://www.springer.com/gp/book/9783540306979)\n",
    "    \n",
    "- For beach reading on Deep Learning, I'd consider John D. Kellher's book [*Deep Learning*](https://mitpress.mit.edu/books/deep-learning-1) through MIT Press."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a Decision Tree?\n",
    "\n",
    "* A decision tree is a locally optimal search algorithm that uses a test function to iteratively split the data set. \n",
    "* A decision tree can be viewed as a graph in which each internal node represents a “test” on an attribute. \n",
    "* Each branch represents the outcome of the test, and each leaf node represents a class label. \n",
    "* The paths from root to leaf represent classification rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"Root.png\" alt=\"A Decision Tree\" width=\"500\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Purpose of Using a Decision Tree\n",
    "\n",
    "* The goal of using a decision tree is to 'clean up' our data. We want to use a given number of assesments to efficiently seperate our data into however many groups necessary, in order to have clean seperation. \n",
    "\n",
    "* If we know what questions to ask in order to separate our data, we have a better chance at making predictions just be using the answers to those questions.\n",
    "\n",
    "* Decision trees fall into the category of **supervised learning algorithms** (having a pre-defined target variable) that is mostly used in **classification problems**, however there are decision tree methods for both categorical and continuous labels, so one can use decision trees for **regression** as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decisions\n",
    "\n",
    "In order to 'clean up' our data, we need to compare the values of the features associated to each data point and then make a decision of cutoff value to seperate our data into two groups, let's say people we want to hire or not hire, and then split up those groups based off of their feature values, and so on...\n",
    "\n",
    "This leads to the natural question, how do we know we've made a good seperation of our data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Entropy\n",
    "\n",
    "* At each step of seperation, we want to decrease how 'messy' our data set looks in order to justify/validate our decision making process. The measure of this disorder is what we call **Entropy**.\n",
    "\n",
    "* For our purposes, another convenient interperetation of **entropy** is a measure of uncertainty in our data, i.e., after making some decision to split my data, how uncertain am I in knowing how to label each data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Entropy\n",
    "The picture below shows a splitting such that we can be more sure that data points on the left and right are *more* similar to each other than they were in the root node.\n",
    "<center>\n",
    "    <img src=\"DecisionTree.png\" alt=\"Decision Tree\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "**Question: If we interpret entropy as \"messiness\", which of the above boxes have high entropy and which have low entropy?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Measuring Entropy\n",
    "\n",
    "Suppose we have a set of $N$ data points and that these points fall into two categories, $n$  have Label 1 and $m = N-n$ have Label 2. To get our data a bit more ordered, we want to group them by labels. We introduce the ratio\n",
    "$$ p = \\frac{n}{N} \\; \\text{and}\\; q = \\frac{m}{N} $$\n",
    "\n",
    "in order to measure how much of the data is labeled as Label 1 and Label 2 in a given node.\n",
    "\n",
    "### Probability of a Class Label\n",
    "\n",
    "One can interpret the above ratios as the probability that a randomly selected point is in class 1 or in class 2. This is an example of a **Bernoulli random variable** because it has only two possible values, with probability $p$ and $(1-p)=q$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Entropy for Data with Two Class Labels\n",
    "\n",
    "The formula we use to calculate the *entropy*, or *uncertainty* in our data is\n",
    "\n",
    "$$ E = -p\\log_2(p) - q\\log_2(q) = -p\\log_2(p)-(1 - p)\\log_2(1 - p) $$\n",
    "\n",
    "Notice that the maximum of this function is 1 and is symmetric about $p = 0.5$. We can interperet this as, when the proportion of our data in a single node is split 50/50 we are the most uncertain how to label the data in that node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd3xUZdr/8c+VCkloKSRICKEaQ8fQsYEilgVZG7ZdC7Zdy7P629UtlnXX1Ud33bXt2tZeVtBVUFGwoYi00AmIhBIIEBISEiAkpF2/P2biE2LKBDJzZjLX+/WaF5k5Z+Z8z4TMNfd9n3MfUVWMMcYErxCnAxhjjHGWFQJjjAlyVgiMMSbIWSEwxpggZ4XAGGOCnBUCY4wJclYIjPGQiCwQkRktWP9mEdkrIodEJM6b2fyJiNwkIp/5+rnm2FkhCCIisl1EytwfTLW3pzx8bos+BL1NRCJE5F4R2SQipSKyS0Q+FpFJTmcDEJFw4DFgkqrGqGrhcb5e3d9ZTb3f4xWtk/pH23xYRF7wxmsb/xLmdADjcz9R1Vb/xiUiYapa1dqv24R3gO7Az4BV7scmAOcB8+uv7EC+RKAdkNXSJ4qIAKKqNbWPqWpMneXbgRne+D2a4GQtAgOAiFwtIt+IyF9FZL+IbBORc9zLHgROAZ6q24oQERWRX4rIZmCz+7GxIrJcRErc/46ts40FIvKQiCxzL58tIrHuZR+JyK31Mq0VkQsayHomcBYwVVWXqmqF+/aJqt5eZ73tInKXiKwFSkUkTETuFpEtInJQRDaIyLR678EiEXnSne87EZlYb/M93escFJH5IhLfQL7+wCb33WIR+cLD9+ZBEVkEHAZ6N/Mrq7u9DiJSLiId3ff/LCJHRKS9+/5fReRh98+xIvKmiBS4f8e/cReeFnG3xra534f1InJevVVCRORZETngfp9PrfPcWBF5VUTyRGSniNwnIj/6LBKRUBF5yp21RETWiMiJLc1qPKCqdguSG7AdOLORZVcDlcD1QChwM7Ab1zdTgAW4voXWfY4CnwKxQHv3v/uBq3C1Ni9z34+r8xq7gIFANPAu8Lp72SXA0jqvPQQoBCIayPowsMDD/V0N9ADaux+7GDgB15egS4FSoFud96AK+BUQ7l5eAsTWyb8F6O/e3wXAw41sO9X9/oS573vy3uwABriXh7fk9wgsA85z//y1O+cZdZad4/55JjALiAH6AtuAKxrZzsPAC40suxTo5n4frwIOAvHuZTe538dfuN/HnwFFQEf38o+BJ4Eo92usAn5e57mfuX+eCiwGOrq3MwDo6vTfUVu8WYsg+LwvIsV1btfXWZajqs+rajXwCq4/0sRmXu8hVS1S1TJc3TKbVfU1Va1S1beA74Cf1Fn/NVVdr6qlwD3AJSISCswG+olIP/d6VwFvq2pFA9uMB/Jq77i/YRa7vzWW11v3CVXd6c6Hqs5S1d2qWqOqb+NqyYyss34+8A9VrXQv3+Ter1ovqer37tebCQxt5v2p5cl787KqZrmXV3r4urW+Ak4TkUigH/Av9/0OwGBgkXvZhcBdqnpIVbOBf+B6r1tEVd9W1T3u9/E1XAX+5Dqr7FTVf7rfx1eBXOBsEekJnArcoaqHVXUP8AQwvYHNVOIqAmmuTWqWqua3NKtpnhWC4HOBqnauc3u+zrIfPlxV9bD7xxiatrPOzycAOfWW5+Dqy29o/Rxc3xjjVfUIrg/WK93dBJcBrzWyzUJcRao2a5Gqdsb1QRTZRD5E5Gcisrq2EOJqndTt3tmlqnVnYsxx71etvDo/H6b596dWS9+blvoKOB0YBWQCXwCnAeOAdap6AEjC9Te/o4kMHhGR69xdd7XvY1+Ofh9z6z2l9n3siWvspKDOcx+n4S8cHwP/Bp4F9orIP0XE0/fbtIAVAuOpxqaprfv4blx/6HWl4Pq2WKtHvWWVwD73/VeAK4CJwGFVXdzINj8HRohIcktyu7+NPg/cgqtLpjOwHqjbR969Xp95inu/jpcn783xTAW8EFd32nm4isJqXN+kJ7nvg6uI1bi321iGZrnHQJ4EbsDVbdYZyObo97H+76b2fdwJHAK61Pky0lFVh9ffjro8pqrDcLVqhgC311/PHD8rBMZTe2l+AHMu0F9ELncPzF4KpAMf1lnnShFJF5Eo4AHgHXdXFO4P/hrgbzTeGkBV5wNf4urmGiWuQ0nDgdHN5IvG9WFbACAi1+BqEdTVFbhNRMJF5GLgJPd+HS9P3ptjpqoluI5Quhn4Sl1HHGUCM3AXAner6z3gLyISLSJ9cH2wvt7ES4eKSLs6twhcraAaXO9jiIjchKtFUFcPcZ0TECYiV+IqBPNVdRuwBHjEPcgdIiL9RGR8/Q2LyGgRyRCRMFxjORVA9bG9Q6YpVgiCzwdy9DHp73n4vMeBi8R1RNETDa2grmPlzwfuxNV98xvgfFXdV2e114CXcX07bQfcVu9lXgUG0fSHE8BPcX2Ivg4U4x70BCY39gRV3YCryCzGVdgGAYvqrbYUVx/7PuBB4CI9znMA3Nv25L05Xl/h+la+ss79aOCbOuvc6P43B1f30QvAG0285tVAWZ3bBlVdCTyDq9DsAXq5f67ra2AYrkHi3wPT3MUKXN1+nXGNkRQBb9Nw11BnXP9XioGt7swN/t8zx0eO7g41xntEZAGuo4QaPUlJRH4G3KCqP/qG6G0icjWuI6N8vm1jnGQtAuM33N1FvwCeczqLMcHECoHxCyJyNq4+573Amw7HMSaoWNeQMcYEOWsRGGNMkAu4Sefi4+M1NTXV6RjGGBNQVqxYsU9VExpaFnCFIDU1lczM+keqGWOMaYqI1D+z/QfWNWSMMUHOCoExxgQ5KwTGGBPkrBAYY0yQs0JgjDFBzmuFQEReFJF8EVnfyHIRkSdEJNs9r/mPpqE1xhjjfd5sEbxMEzNBAufgmuWxH655zf/lxSzGGGMa4bXzCFT1axFJbWKVqcCr7qtBLRGRziLSzX3pOmP8yuGKKopKK350O1BeBQ1N0yJCp/bhxEaHExsdSWxUBLExEcRGRdA+ItT3O2BME5w8oaw7R1+aL9f92I8KgYjcgKvVQEpKSv3FxrQKVWV3STnZ+Yd+uG3JP0R2wSGKShu6dLLLUdcz++G1Gt9OfEwEfRJi6Nv16FtSx3ZIQy9mjJc5WQga+h/f4J+Pqj6He2rijIwMmyXPtIqq6hqydh9g2bYilm4rIjOniOLD/3fN+M5R4fRNiGFSeiIpcVHER0fSJTqC2Dq3ju3CGvzwrqlRDpZXUVh6hP2HKyg8VMH+wxXsO1RBTmEp2fmH+GDNbleLwi0uOoIRqbGM6BXLqF6xnNStI6EhVhiM9zlZCHI5+vq1ybTOtWGNaVR2/kHmZe1lydZCVuTs53CF68qHqXFRTEpPZFByZ/q5v6HHRUcc8zf0kBChU1Q4naLCG11HVSk4dOSH1seanSUs217IJ1l5AHSIDOPk1C6M6R3H2QOSSI2PPqYsxjTHq9NQu8cIPlTV+teFRUTOw3UR8XOBUcATqjqyudfMyMhQm2vIeEpVydp9gE/W5/FJVh7Z+YcASEvqwMhesa5baixdO7ZzOOn/2V1cxvLtRSzb5rptrpN58sAkJg9M4sTEDtaNZFpERFaoakaDy7xVCETkLeB0IB7XxUbuA8IBVPUZcf0vfgrXkUWHgWtUtdlPeCsExhOb9x5k1opcPl6/h51FZYQIjOoVxzmDkpiUnkRSJ//54G/OruIyPlmfx7z1eSzPKUIVesVHM3lgEpdk9KCXtRSMBxwpBN5ihcA05khVNZ+sz+ONpTtYtq2I8FBhfN94Jg9M4qz0JGKjI5yOeNzyD5bz6Ya9fLI+j2+3FFJdo4zrG8cVo3pyVnoi4aF2jqhpmBUC06Zt31fKW8t2MGtFLkWlFfSMi+LykSlcdHIycTGRTsfzmvyD5czKzOXNpTvYVVxGQodILs3owfSRPUjuEuV0PONnrBCYNmldbglPfLGZTzfsJTREmJSeyOWjUhjXJ56QIDraprpG+fr7At5YmsMX3+WjwLmDunHrhL6kJXV0Op7xE00VgoC7MI0xK3L28+QXm1mwqYCO7cK4bWI/rhiVQqIfDfj6UmiIcEZaV85I68ru4jJeW5LDa4tz+GjtHialJ3LbxH4M7N7J6ZjGj1mLwASMpVsLefKLbL7J3keXqHBmnNKbn43pSYd2jR+iGayKD1fw0qLtvLRoGwfKqzjjxARundiP4SldnI5mHGJdQyagbco7yJ8/2sDCzfuIj4nkhlN7ccWonkRHWoO2OQfKK3n12+288M02ig9XMiGtK3847yR6J8Q4Hc34mBUCE5D2l1bw2Kff88bSHGIia7uAetpcPceg9EgVL3+7nX8t2EJ5ZTVXj03l1on96NTeWlPBwgqBCSiV1TW8viSHf3y2mYPllVw5uie/OrM/XdrA4Z9Oyz9Yzt/mfc/MFTvpEhXBnZP6M31Eik1lEQSsEJiAsXBzAX/8YAPZ+YcY3zeee85P58SkDk7HanPW7yrhgQ82sGx7EWlJHfjjlAGM6h3ndCzjRVYIjN8rKavkzx9uYNaKXFLjovj9eemceVJXm0bBi1SVuevy+MvcjewqLuOq0T25+5w0G3tpo+zwUePXvvhuL7/97zr2HargF6f34baJ/WgXbuMA3iYinDe4GxPSuvLX+Zt4cdE2vtyUz/9eOJhxfeOdjmd8yM5HN44pOVzJHTNXc+3LmXRuH8H7vxjHbyanWRHwsfYRodxzfjqzbhxDRGgIV7ywlN+9t46D5ZXNP9m0CdYiMI74bMNefvfeOopKK7htQl9+OaEvkWFWAJyUkRrL3NtP4bFPv+eFhVv5alMBD184iFP6JTgdzXiZtQiMTx2pqua+2euZ8WomcTGRvP/Lcdwx6UQrAn6iXXgovzv3JN65eSztwkO46t/LePjj76isrnE6mvEiaxEYn9lReJhb3lrJ2twSZozvxW8mpxERZt9F/NHwlC58dNsp/OnDDTzz1RYytxfx5OXD6NapvdPRjBfYX6HxiU/W53HekwvZvq+U5646mT+cn25FwM+1Cw/lwWmDeHz6UDbuOcB5T3zDgk35TscyXmB/icarKqpq+OMHWdz0+gp6x0fz0W2nMGlAktOxTAtMHdqdObeOp2uHSK5+aTmPzvuOKusqalOsEBiv2VNSxsXPLualRdu5Zlwqs24aS49Ymyc/EPVJiOH9X45j+ogePP3lFi5/YSkFB484Hcu0EisExivW5hYz9alFbMk/xL+uGM59PxlgXUEBrl14KA9fOJi/XzqEtbnFXPD0IjblHXQ6lmkF9pdpWt0n6/dwybOLCQ8N4d2bx3LOoG5ORzKtaNqwZGbeOIbK6hou/Ne3Nm7QBlghMK1GVfnngmxuen0lJ3XryPu/HGfzBLVRg5M7M/uWcaTERnHty8t55dvtTkcyx8EKgWkVFVU1/PqdtTzyySamDDmBt64fTUKHtnu9YAPdOrVn1k1jmJCWyH1zsrh39nobRA5QVgjMcSs+XMFV/17KOytyuX1iPx6fPtSmiQgS0ZFhPHvVyVx/Si9eXZzDda9k2tQUAcgKgTku+QfKueTZxazaUczj04fyq7P624yhQSY0RPj9eek89NNBLMrex+XPL6WotMLpWKYFrBCYY7az6DAXP7uYXfvLePnaEUwd2t3pSMZBl41M4bmfncz3ew9y6bOL2Xug3OlIxkNWCMwxyc4/xMXPLKb4cCWvzxjF2D42bbGBCWmJvHzNSHYXl3HxM4vZWXTY6UjGA1YITIut31XCpc8upqpG+c8NoxmW0sXpSMaPjOkTxxvXj6akrJKLn1lMdr6da+DvrBCYFlmRU8Rlzy+hXXgos24aw0ndOjodyfihoT068/aNo6mqUS55dgnrd5U4Hck0waNCICJdRGSAiPQWESseQWpR9j6ufGEZ8TGRzLxpDL3io52OZPxYWlJHZt00hvbhoVz23BIytxc5Hck0otEPdRHpJCK/E5F1wBLgWWAmkCMis0TkDF+FNM5burWQ615ZTs+4KGbeOIbunW06YtO8XvHRzLppDAnuCetW7yx2OpJpQFPf7t8BdgKnqOqJqjpeVTNUtQfwMDBVRK7zSUrjqFU79nPty8tJ7hLFGzNG2YlipkVO6NyeN68fTZfocH7+4jI27jngdCRTj6iq0xlaJCMjQzMzM52OETSydpdw2XNL6BIdwcwbx5DYsZ3TkUyA2ll0mEueXUxldQ3/uWEMfbvGOB0pqIjIClXNaGhZs/399b/1i0ioiNzXWuGM/8rOP8hV/15GTGQYb8wYZUXAHJcesVG8PmMUAFe+sNQOLfUjngz8ThSRuSLSTUQG4hovsJnE2ricwlIuf34pISK8cf1okrvYdQTM8euTEMPrM0ZRVlnNZc8vYU9JmdORDB4UAlW9HHgFWAfMBf5HVf+ft4MZ5+wqLuPy55dSWV3DGzNG2dFBplWlJXXk1WtHUny4kiuetwvc+ANPuob6AbcD7wLbgatExKOvhyIyWUQ2iUi2iNzdwPIUEflSRFaJyFoRObeF+U0r219awVUvLOVAWSWvXjvKppE2XjGkR2deumYEe0rK+dmLy2yiOod50jX0AXCPqt4InAZsBpY39yQRCQWeBs4B0oHLRCS93mp/AGaq6jBgOvDPFmQ3ray8spobXsskt7iMF68ZwaDkTk5HMm3YiNRYnrnKNTfRL99cRaVNYe0YTwrBSFX9HEBd/gZc4MnzgGxV3aqqFcB/gKn11lGg9tTUTsBuz2Kb1lZTo/z6nbUs376fv108hBGpsU5HMkHgtP4J/GXaQL7+voB73l9PoB3F2FaENbeCqv7ooF9V3ezBa3fHdR5CrVxgVL117gfmi8itQDRwZkMvJCI3ADcApKSkeLBp01J/nb+JD9bs5u5z0vjJkBOcjmOCyKUjUthZVMZTX2aTEhfFL07v63SkoOPN6SIampS+frm/DHhZVZOBc4HXGprCQlWfc5/MlpGQkOCFqMHtzaU7+OeCLVw+KoUbT+3tdBwThO6c1J+pQ0/gkU82MXv1LqfjBB1vFoJcoEed+8n8uOvnOlzTVqCqi4F2gM1n7EMLNuVzz+z1nNY/gQemDLCLyhhHiAiPXDSYkb1i+fWstSzbZvMS+VKLCoGI3N+C1ZcD/USkl4hE4BoMnlNvnR3ARPdrn4SrEBS0JJM5dht2H+CXb6ykf2IHnr5iOGGhNp+gcU5kWCjPXXUyybHtueG1TLYUHHI6UtBo6V/+FE9XVNUq4BZgHrAR19FBWSLygIjUvs6dwPUisgZ4C7habbTIJ/IPlHPty8vp0C6cl64eQUxks8NFxnhd56gIXr56JKEiXPPScvbbJS99okVzDYnIKvehno6xuYaOX0VVDZc/v4Ss3Qd49+axpJ9g1xQw/mXljv1Mf3YJo3rH8vI1IwkNsS7L43Vccw3Vc3Ir5DEOe/CjDWTm7Od/LxpsRcD4peEpXXhg6gAWbt7HY59ucjpOm9eiQqCqdsZHgHt3RS6vLM5hxvheTLHDRI0fmz4yhctG9uDpL7fwyfo8p+O0aTY6GETW7yrhd++tY3TvWO4+J83pOMY06/4pAxjSozN3zlxNdr4NHnuLFYIgUVRawY2vrSA2OoKnLrcjhExgiAwL5Zkrh9MuPJQbXsu0OYm85Jg+DUTkmtYOYrynuka57a1VFBw8wjNXnkx8jF1hzASObp3a89Tlw8kpPMydM9dQU2MHFra2Y/1a+MdWTWG86tF5m/gmex9/usDVzDYm0IzpE8dvz0lj/oa9/OurLU7HaXMaPXhcRNY2tghI9E4c09rmZ+XxzFdbuGxkCpeOsHmaTOC6bnwv1uSW8Nf5mxjaozPj+tokBK2lqbOIEoGzgf31HhfgW68lMq1mT0kZv3l3LQO7d+T+KfVnADcmsIgI/3vhIDbsLuFXb6/m49tPIc66OVtFU11DHwIxqppT77YdWOCTdOaYVdcov3p7NRVVNTwxfRiRYaFORzLmuEVFhPHkZcMpPlzJXe+utWmrW0mjhUBVr1PVbxpZdrn3IpnW8MxXW1iytYj7pwygd0KM03GMaTXpJ3Tk7nPS+GxjPq8uznE6TpvQaCEQkWY/PTxZx/jeyh37eezT7zl/cDcuPjnZ6TjGtLprxqVyxokJPDh3Ixv3/OiSKaaFmuoami0ifxORU0Xkh6uXi0hvEblOROYBk70f0bTEwfJKbv/PKpI6tuPBaYNsWmnTJokIj148hI7twrntrVWUV1Y7HSmgNdU1NBH4HLgRyBKREhEpBF4HkoCfq+o7volpPHXP++vZtb+Mx6cPpVP7cKfjGOM18TGRPHbJEDbnH+LPH21wOk5Aa3LuYVWdC8z1URZznP67Mpf3V+/mV2f2J8OuOWyCwKn9E7j+lF48v3Abp/RL4OwBSU5HCkjNnlAmIj/6WikidgCvn8kpLOWe99czMjWWWybYNV9N8Pj12WkM7N6Ru95dy56SMqfjBKSmBovPEJFcYLeIzBeRXnUWz/d+NOOp6hrlzplrCA0R/j59qM3dboJKRFgIT0wfRkVVDXe9u84OKT0GTbUIHgHOVtUE4DlgvoiMdi+zTxo/8sq328nM2c99PxlA987tnY5jjM/1TojhrslpfP19AbNW5DodJ+A0VQgiVDULwD0ofAHwiohMA6zk+omcwlIemfcdZ5yYwE+Hd3c6jjGOuWp0T0b2iuVPH24gr6Tc6TgBpalCUCkiP4y8uIvCROA+oJ+3g5nm1dQod727lvCQEP7yUztU1AS3kBDhkQsHU1ldw+/fsy6ilmiqENxNvcnlVDUXOB142IuZjIfeWLaDJVuL+MP5J9Gtk3UJGZMaH83/m3Qin3+Xz+zVu52OEzCaOo/gM1Vd08Djxar6oHdjmebk7j/Mw3M3ckq/eC7J6OF0HGP8xjXjejE8pTP3f5BF/kHrIvKEJ4ePrhORtfVuC0Xk7yIS54uQ5miqym//uw6Ah6xLyJijhIYIj1w0hMMV1dz7fpZ1EXnAkwvTfAx8BFzhvn0ALATygJe9lsw06u3lO1m4eR93n3sSyV2inI5jjN/p2zWGO87qzydZeXy0bo/Tcfxek2cWu41T1XF17q8TkUWqOk5ErvRWMNOwPSVlPPjRRkb3juWKkXahGWMaM2N8Lz5et4d7Z2cxpnecXbugCZ60CGJEZFTtHREZCdTOOlrllVSmQarKH95bT1WN8r8XDibEThwzplFhoSE8ctEQDpZX8sCHNhdRUzwpBDOAF0Rkm4hsB14AZrhnJH3Im+HM0T7bmM/n3+Vzx1n96RkX3fwTjAlyJyZ14Ben92X26t18u2Wf03H8VrOFQFWXq+ogYCgwVFUHux8rVdWZ3o9oAMoqqrl/Thb9E2O4elyq03GMCRg3n96HlNgo7p2dRUVVjdNx/JInRw11EpHHcE1J/Zn7GgWdvB/N1PXPBdnsKi7jgakDCQ/1pCFnjAFoFx7K/VPSyc4/xEuLtjkdxy958onyInAQuMR9OwC85M1Q5mjb9pXy7FdbmTasO6N72xG7xrTUhLREzkpP5PHPN9sMpQ3wpBD0UdX7VHWr+/ZHoLe3gxkXVeXe2euJDAvht+emOR3HmIB17/np1Kjy5w83Oh3F73hSCMpEZHztHREZB1hJ9ZFP1uexcPM+7pjUn64d2jkdx5iA1SM2ilvO6MtH6/bw9fcFTsfxK54UgpuBp0Vku4jkAE8BN3k3lgE4XFHFAx9u4KRuHblqdE+n4xgT8K4/tTepcVHcPyeLI1V2neNanhw1tFpVhwCDgUGqOqyhOYhM63vi82z2lJTz5wsGEGYDxMYct8iwUP44dSBb95XywkIbOK7V6JnFInJHI48DoKqPeSmTAbLzD/LCwq1cfHIyJ/e06w8b01pO65/AOQOTePKLzUwdeoJN00LTLYIOzdyaJSKTRWSTiGSLyN2NrHOJiGwQkSwRebNl8dsmVeW+OVlERYRy1zk2QGxMa7vn/HQE4YEP7IxjaKJF4D466JiJSCjwNHAWkAssF5E5qrqhzjr9gN/ims9ov4h0PZ5tthWfb8xnUXYhf5wygHibH8WYVndC5/bcMqEvj87bxJKthUF/WHaTHc8i0k5ELhKRx0Vkloi8KiK/EZEBHrz2SCDbfchpBfAfYGq9da4HnlbV/QCqmn8sO9GWVFXX8NDHG+kdH83lo2xSOWO85brxvTihUzv+MncjNTXBPVV1o4VARO4HvgXGAEuBZ4GZuCaae1hEPhWRwU28dndgZ537ue7H6uoP9BeRRSKyREQmN5LlBhHJFJHMgoK2fdjX25k72VJQyl3npNkZxMZ4UbvwUO6cdCJrc0v4YG1wX82sqWmol6vq/Y0se8zdjdPUV9aGpsasX3bDcF3/+HQgGVgoIgNVtfioJ6k+BzwHkJGR0WZL96EjVfz9082MSO3CpPTE5p9gjDku04Z159/fbOPReZuYPDCJyLBQpyM5oqlLVX4Eru6h+stEJF5V81U1s4nXzgXqXkMxGahfdnOB2apaqarbgE24CkNQeu7rrew7dITfnXuSXXXMGB8ICRF+d+5J5O4v49Vvc5yO4xhP+h6Wi8jo2jsiciGuLqNmnwf0E5FeIhIBTAfm1FvnfeAM9+vG4+oq2upJ8LZm74Fynv96K+cN7sawlC5OxzEmaIzvF89p/RN48ovNFB+ucDqOIzwpBJcDT4rIoyLyBq4B3gnNPUlVq4BbgHnARmCmqmaJyAMiMsW92jygUEQ2AF8Cv1bVwmPZkUD390+/p6qmhrvOtsNFjfG1356bxqEjVTz1RbbTURzR7KUqVXWdiDwIvIZrFtJTVTXXkxdX1bnA3HqP3VvnZwXucN+C1vd7DzIzcydXj+1FSpyd3GKMr6UldeSik5N5dXEOPx+bSo/Y4Po79OR6BP8G/gfXFBPXAB+IyC+9HSyYPDR3I9GRYdw6oa/TUYwJWnecdSIhIfDIvE1OR/E5T7qG1gNnqOo2VZ0HjAaGezdW8Pg2ex9fbirgljP60iU6wuk4xgStpE7tuP6U3nywZjdrdhY3/4Q2xJNJ5/7u7sKpvV+iqtd5N1ZwqKlRHpy7ke6d2/PzsalOxzEm6N14WmVXbHYAABd4SURBVB/iYyJ4cO5G6nzstXl2xpKD5q7fQ9buA/z67BNpFx6cxy8b409iIsO4/cz+LNtWxIIgumaBFQKHVNcoj3+2mX5dY/jJkBOcjmOMcbs0owfdO7fnH59tDppWQYsKgYgkeStIsJm7bg+b8w9x+5n9CA2xk8eM8RcRYSHcOqEva3YWs2BTcLQKWtoimNv8KqY51TXK459vpn9iDOcO7OZ0HGNMPReenExyl/b847Pvg6JV0NJCYF9dW8FH6/aQnX+I2yf2J8RaA8b4nfBQd6sgt4QvN7X9SZFbWgie90qKIOIaG/ieExM7cM5A62kzxl/9dHgyPWKDY6ygRYVAVf/prSDB4sO1u9lSUMrtZ/az1oAxfiw8NIRbz+jH2twSvviubbcK7KghH6quUZ74fDNpSR2YPMBaA8b4u2nDu5MSG9XmWwVWCHzoh9bARGsNGBMIwkNDuGVCX9btKuHzjW23VWCFwEdqjxRKS+rA2dYaMCZgTBvmbhV83naPIDqmQiAi61o7SFv3wZrdbC0o5X9sbMCYgFJ7BNH6XQf4rI22ChqdhlpEftrYIsC+0rZAVXXND2MDk9LtrTMm0Ewb1p2nvszmH599z5kndW1zVxBs6noEbwNv8OPrDAP86PKVpnEfrt3D1n2lPHPlcGsNGBOAwkJDuOWMvvz6nbV8tjGfs9rYNcWbKgRrgb+q6vr6C0TkTO9FaltUlWe/3krfrjHWGjAmgE0b1p1/fLaZ577e0uYKQVNjBP8DHGhk2TQvZGmTvsnex8Y9B7jhlN7WGjAmgIWFhnDt+F4s376flTv2Ox2nVTVaCFR1oaruaGRZpvcitS3Pfb2VhA6RTB1mM4waE+guHdGDju3CeP7rrU5HaVWNFgIR+YOIxDaxfIKInO+dWG3Dht0HWLh5H1ePTSUyzK43YEygi4kM44rRPfkkK4+cwlKn47SaprqG1uG6PvHnIvKoiPxGRO4Vkdfch4/+BFjqm5iB6YWFW4mKCOXKUT2djmKMaSXXjE0lLER4YeE2p6O0mqa6hmar6jjgJiALCMU1ZvA6MFJVf6WqwTFZ9zHYXVzGnDW7uXREDzpFhTsdxxjTSrp2bMcFQ7sza8VOikornI7TKpo6aggAVd0MbPZBljbl5W+3o8B143s5HcUY08puOLU3s1bk8triHG4/s5/TcY6bTTHhBQfKK3lz6Q7OG9SN5C5RTscxxrSyfokdmJDWlVcXb6e8strpOMfNCoEXvLV0B4eOVHHDqb2djmKM8ZLrT+lNYWkF767MdTrKcbNC0Moqqmp4adF2xvaJY2D3Tk7HMcZ4yejesQxO7sQLC7dRUxPYk9G19OL1K70VpK34YM1u8g6Uc721Boxp00SE60/pzbZ9pXy6ca/TcY6LXbO4Fakqzy/cyomJHTi9f4LTcYwxXnbOwCSSu7QP+BPMWloIPvJKijbi6837+C7vIDNO6dXmZic0xvxYWGgI143vRWbOflbkBO60Ey29ZvEfvBWkLXjxm2107RDJ1KHdnY5ijPGRSzJc0068uChwTzCzweJWsqPwMF9vLuCykSlEhNnbakywiI4M4+KMHsxbn0f+wXKn4xwT+8RqJW8u20GICNNH9nA6ijHGxy4flUJVjTIrMzAPJbVC0AqOVFUzM3MnE9O60q1Te6fjGGN8rE9CDGP7xPHm0h1UB+ChpE0WAhEZIyJPi8haESkQkR0iMldEfikidpC82yfr8ygqreDK0Ta5nDHB6srRPdlVXMaCTYF3XeOmpqH+GJgBzAMmA92AdOAPuC5VOVtEpjT14iIyWUQ2iUi2iNzdxHoXiYiKSMax7ITTXl+SQ8+4KMb3jXc6ijHGIWelJ9K1QySvL8lxOkqLNTXp3FWquq/eY4eAle7b30Sk0U8+EQkFngbOAnKB5SIyR1U31FuvA3AbATql9Xd5B1i+fT+/PSfNrkBmTBALDw1h+ogePPllNjuLDtMjNnDmGWtqGup9ACKSXn+ZiJxed51GjASyVXWrqlYA/wGmNrDen4BHgIAcbn9jyQ4iwkK4OMMGiY0JdtNHpiC4Dh4JJJ4MFs8UkbvEpb2IPAk85MHzugM769zPdT/2AxEZBvRQ1Q+beiERuUFEMkUks6DAfy6BUHqkivdW7eK8Qd2IjY5wOo4xxmEndG7PhLREZi7fSUVVjdNxPOZJIRgF9AC+BZYDu4FxHjyvoX6SH4bTRSQE+DtwZ3MvpKrPqWqGqmYkJPjP1A2zV+/m0JEqrhyd4nQUY4yfuHJ0CoWlFXySled0FI95UggqgTKgPa5B4m2q6kmpy8VVQGol4yoitToAA4EFIrIdGA3MCZQBY1Xl9SU5pCV1YHhKF6fjGGP8xKn9EkiJjQqoQWNPCsFyXIVgBDAeuExE3vHwef1EpJeIRADTgTm1C1W1RFXjVTVVVVOBJcAUVc1s6U44YdXOYjbsOcAVo3vavELGmB+EhAiXj0ph2bYivt970Ok4HvGkEFynqveqaqWq5qnqVGB2c09S1SrgFlyHn24EZqpqlog80Nxhp4Hg9SU5REeEMm2YzStkjDnaxScnExEawhsB0ipo6jyCGICGvqGr6mt112mMqs5V1f6q2kdVH3Q/dq+qzmlg3dMDpTVQfLiCD9fu4YJh3YmJbPayz8aYIBMXE8m5g5L478pdHK6ocjpOs5pqEcwWkb+JyKkiEl37oIj0FpFrRaT2RLOg886KXCqqarhilJ1JbIxp2BWje3LwSBVzVu9ufmWHNXUewUTgc+BGIEtESkSkEHgd11nGP1dVT8YK2hRV5c1lOxie0pn0Ezo6HccY46cyenYhLalDQJxT0GS/hqrOBeb6KEtAWJNbwtaCUh766SCnoxhj/JiIcNHJyfz5o41k5x+ib9cme9Id1exgsYiEN/BY0E6q897KXCLCQjh3UDenoxhj/NyUIScQIvDeKv+enrqpweIzRCQX2C0i80WkV53F870fzf9UVtfwwdo9nHlSVzq1/1F9NMaYo3Tt2I7x/RJ4f9Vuavx4euqmWgSPAGeragLwHDBfREa7lwXlgfNfbSqgqLSCacOSnY5ijAkQPx3WnV3FZSzbXuR0lEY1VQgiVDULwD0ofAHwiohMo85UEcHkvdW76BIVzmn9/WeaC2OMf5s0IJGoiFDeX7XL6SiNaqoQVIpIUu0dd1GYCNwH9PN2MH9zoLySTzfs5SdDTrBrEhtjPBYVEcbkgUl8tG4P5ZXVTsdpUFOfaHcDiXUfUNVc4HTgYS9m8ksfr9tDRVWNnUlsjGmxnw5L5mB5FZ9v9M+rlzV1HsFnqrqmgceLa88SDib/XbmLXvHRDO3R2ekoxpgAM6ZPHIkdI/326KFm50cQkXX8eEygBMgE/qyqhd4I5k9y9x9m6bYi7jirv00wZ4xpsdAQ4YKh3fn3N9soPHSEuJhIpyMdxZPO7o+Bj4Ar3LcPgIVAHvCy15L5kdnuU8StW8gYc6ymDe9OVY3y4do9Tkf5EU9mTBunqnUvRLNORBap6jgRudJbwfyFqvLflbmMSO0SUNcgNcb4l7SkjpzUrSP/XbWLn49NdTrOUTxpEcSIyKjaOyIyEqg9V9r/p9U7Tut2lbCloNTOHTDGHLdpw05gzc5ithQccjrKUTwpBDOAF0Rkm/tKYi8AM9wzknpy7eKA9t+Vu4gIDeE8m1LCGHOcpg7tTojgd+cUNFsIVHW5qg4ChgJDVXWw+7FSVZ3p/YjOqayu4YM1u5l4Ulc6RdmUEsaY45PYsR3j+sbz3qpdfjXlhCeTznUSkcdwTUn9mfsaBZ28H815CzcXUFhaYYPExphWM21Yd3L3l5GZs9/pKD/wpGvoReAgcIn7dgB4yZuh/MV/V7qmlDj9xK5ORzHGtBFnD0iifXioX51T4Ekh6KOq96nqVvftj0BvbwdzWumRKj7dsJfzB9uUEsaY1hMd6Zpy4sO1rtkK/IEnn3BlIjK+9o6IjAPKvBfJP3z1fQFHqmo4b7ANEhtjWtd5g7pxsLyKJVv943xcT84juBnXrKOdcE0/XQRc7c1Q/mB+Vh5dosLJ6NnF6SjGmDZmfL94oiJCmb8hj1P9YDZjT44aWq2qQ4DBwCBVHdbQHERtSUVVDZ9/l8+ZJyUSFmrdQsaY1tUuPJTT+icwP2uvXxw91GiLQETuaORxAFT1MS9lctzSbYUcLK9i0oCk5lc2xphjMGlAIh+vz2NNbjHDUpzteWjq626HZm5t1rysPNqHh3JKv6C9NLMxxssmnJhIWIgwL2uv01EabxG4jw5qkIhEeCeO82pqlE837OW0/gm0Cw91Oo4xpo3qFBXO6N5xzN+Qx93npDmaxZMTyhaISGqd+yOA5V7M5Ki1u0rYe+AIkwYkNr+yMcYch0kDEtlaUEp2vrNzD3kyEvoQ8ImI/EJEHgSeBa7xbiznzMvKIzREmJhmhcAY411npbs+Z+Zl5Tmaw5OjhuYBNwGPA9cC56rqSm8Hc8r8rDxG9461uYWMMV7XrVN7hiR3Yv4GZ8cJPOkaugd4EjgVuB9YICLneTmXI7LzD7GloJRJ6Xa0kDHGNyYNSGLNzmLySsody+BJ11A8MFJVF6vqs8DZwP94N5Yz5m9wNc9qm2vGGONtZ7vHIz/d4Fz3kCddQ7erahmAiCSpao6qnuX9aL43P2svg5M7cULn9k5HMcYEiT4JMfSOj3a0e6ilp83O9UoKP7D3QDmrdxYzyVoDxhgfEhHOGpDI4i2FlJRVOpKhpYVAvJLCD9RW47PtbGJjjI+dPSCJqhrly+/yHdl+SwvB815J4QfmZ+XRKz6avl1jml/ZGGNa0dDkziR0iPxhnNLXWlQIVPWfLVlfRCaLyCYRyRaRuxtYfoeIbBCRtSLyuYj0bMnrt5aSskoWbylkUnriD3MpGWOMr4SECGelJ7JgUwHlldW+3763XlhEQoGngXOAdOAyEUmvt9oqIENVBwPvAI94K09TFmzKp6pGbZI5Y4xjzh6QxOGKahZl7/P5tr05x/JIINt9VbMK4D/A1LorqOqXqnrYfXcJkOzFPI2an7WX+JhIhvXo7MTmjTGGMb3j6BAZxnwHJqHzZiHoDuyscz/X/VhjrgM+bmiBiNwgIpkikllQUNCKEaG8spoFm/I5Kz2RkBDrFjLGOCMiLITT07ry2ca9VPv4GgXeLAQNfao2uHciciWQATza0HJVfU5VM1Q1IyGhda/ms3RbEaUV1XbYqDHGcZPSEyksrWD1zv0+3a43C0Eu0KPO/WRgd/2VRORM4PfAFFU94sU8Dfp2yz7CQ4VRvWN9vWljjDnKuL6ua6As3uLbaxl7sxAsB/qJSC/39QumA3PqriAiw3DNZjpFVR05gHbJlkKGJHcmKsKTyzcbY4z3xEZHkJbUgcU+vqi91wqBqlYBtwDzgI3ATFXNEpEHRGSKe7VHgRhgloisFpE5jbycVxwor2TdrhLG9Inz5WaNMaZRo3vHkbl9P0eqfHcYqVe/BqvqXOpNS6Gq99b5+Uxvbr85y7cVUaOu0XpjjPEHY/rE8fK321m9o5hRPvps8mbXkN9bvKWQiNAQhvd09sLRxhhTa3SvOETwafdQcBeCrYUMS+ls1yY2xviNTlHhpHfr6NMB46AtBMWHK9iw5wBj+8Q7HcUYY44ytk8cq3YU+2y6iaAtBEu3FaGKDRQbY/zOmD5xVFTXsDLHN+cTBG0hWLylkHbhIQzp0cnpKMYYc5QRqbGEhojPxgmCthAs2VpIRs9YIsNsfMAY4186tAtnYPdOfOujcYKgLASFh47wXd5B6xYyxvitMb3jWLOzmNIjVV7fVlAWgqXbigDXiRvGGOOPxvSJo6pGyfTBOEFQFoLFWwqJighlcLKNDxhj/FNGzy6EhYhPDiMNzkKwtZARqbGEhwbl7htjAkB0ZBhDenT2yYBx0H0S5h8sJzv/kI0PGGP83pjecazfVcLB8kqvbifoCsGSra7xAZtfyBjj78b0iaO6Rlm+vcir2wm6QrB4SyEdIsMYcEJHp6MYY0yTTu7ZhYjQEK+PEwRdIViytZCRvWIJs/EBY4yfaxceytAU748TBNWn4Z6SMrbtK7XxAWNMwBjTO46s3QcoOey9cYKgKgS1zSs7f8AYEyjG9IlDFZZu816rIOgKQaf2rilejTEmEAxL6UxkWIhXu4eCqxBsLWRUr1hCQsTpKMYY45HIsFBO7tnFqwPGQVMIdhYdJnd/GWNtfMAYE2DG9onju7yDFB464pXXD5pCUNusGmMXojHGBJjaA1xq50lrbUFTCLpERXBWeiL9E2OcjmKMMS0yOLkzE9K6Eh0Z5pXXF1X1ygt7S0ZGhmZmZjodwxhjAoqIrFDVjIaWBU2LwBhjTMOsEBhjTJCzQmCMMUHOCoExxgQ5KwTGGBPkrBAYY0yQs0JgjDFBzgqBMcYEuYA7oUxECoCcFjwlHtjnpTj+zPY7+ATrvtt+e6anqiY0tCDgCkFLiUhmY2fTtWW238EnWPfd9vv4WdeQMcYEOSsExhgT5IKhEDzndACH2H4Hn2Ddd9vv49TmxwiMMcY0LRhaBMYYY5pghcAYY4JcmykEIjJZRDaJSLaI3N3A8kgRedu9fKmIpPo+ZevzYL/vEJENIrJWRD4XkZ5O5Gxtze13nfUuEhEVkTZxeKEn+y0il7h/51ki8qavM3qDB//PU0TkSxFZ5f6/fq4TOVubiLwoIvkisr6R5SIiT7jfl7UiMvyYNqSqAX8DQoEtQG8gAlgDpNdb5xfAM+6fpwNvO53bR/t9BhDl/vnmYNlv93odgK+BJUCG07l99PvuB6wCurjvd3U6t4/2+zngZvfP6cB2p3O30r6fCgwH1jey/FzgY0CA0cDSY9lOW2kRjASyVXWrqlYA/wGm1ltnKvCK++d3gIkiIj7M6A3N7reqfqmqh913lwDJPs7oDZ78vgH+BDwClPsynBd5st/XA0+r6n4AVc33cUZv8GS/Fejo/rkTsNuH+bxGVb8Gmrpi/VTgVXVZAnQWkW4t3U5bKQTdgZ117ue6H2twHVWtAkqAOJ+k8x5P9ruu63B9ewh0ze63iAwDeqjqh74M5mWe/L77A/1FZJGILBGRyT5L5z2e7Pf9wJUikgvMBW71TTTHtfQzoEFhrRbHWQ19s69/XKwn6wQaj/dJRK4EMoDTvJrIN5rcbxEJAf4OXO2rQD7iye87DFf30Om4Wn8LRWSgqhZ7OZs3ebLflwEvq+rfRGQM8Jp7v2u8H89RrfK51lZaBLlAjzr3k/lx0/CHdUQkDFfzsakmVyDwZL8RkTOB3wNTVPWIj7J5U3P73QEYCCwQke24+k7ntIEBY0//n89W1UpV3QZswlUYApkn+30dMBNAVRcD7XBNytbWefQZ0Jy2UgiWA/1EpJeIROAaDJ5Tb505wM/dP18EfKHu0ZYA1ux+u7tInsVVBNpCfzE0s9+qWqKq8aqaqqqpuMZGpqhqpjNxW40n/8/fx3WAACISj6uraKtPU7Y+T/Z7BzARQEROwlUICnya0hlzgJ+5jx4aDZSo6p6Wvkib6BpS1SoRuQWYh+sIgxdVNUtEHgAyVXUO8G9czcVsXC2B6c4lbh0e7vejQAwwyz02vkNVpzgWuhV4uN9tjof7PQ+YJCIbgGrg16pa6Fzq4+fhft8JPC8iv8LVNXJ1G/iih4i8haubL949/nEfEA6gqs/gGg85F8gGDgPXHNN22sB7ZYwx5ji0la4hY4wxx8gKgTHGBDkrBMYYE+SsEBhjTJCzQmCMMUHOCoExxgQ5KwTGGBPkrBAYc5xEZIR7Lvh2IhLtvg7AQKdzGeMpO6HMmFYgIn/GNa1BeyBXVR9yOJIxHrNCYEwrcM+BsxzXtQ/Gqmq1w5GM8Zh1DRnTOmJxzenUAVfLwJiAYS0CY1qBiMzBdeWsXkA3Vb3F4UjGeKxNzD5qjJNE5GdAlaq+KSKhwLciMkFVv3A6mzGesBaBMcYEORsjMMaYIGeFwBhjgpwVAmOMCXJWCIwxJshZITDGmCBnhcAYY4KcFQJjjAly/x+f41PuuIFF6AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "fig=plt.figure()\n",
    "ax=fig.add_subplot(111)\n",
    "x = np.linspace(0.01,.99)\n",
    "ax.plot(np.linspace(0.01,.99),-x*np.log2(x) - (1 - x)*np.log2(1 - x))\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"-xlog2(x) - (1 - x)log2(1 - x)\")\n",
    "plt.title('Entropy Graph for Two Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Measuring Entropy\n",
    "\n",
    "In general, for $n$ classes $C_1, \\cdots, C_n$ with $p_i$ the proportion of data labeled as class $C_i$, we define the **entropy** as:\n",
    "\n",
    "$$ E(S) = - \\sum_{i=1}^n p_i \\log_2(p_i) = − p_1\\log_2(p_1) − \\cdots − p_n\\log_2(p_n) $$\n",
    "\n",
    "This means the entropy will be small when every $p_i$ is close to 0 or 1 (i.e., when most of the data is in a single class), and it will be larger when many of the $p_i$’s are not close to 0 (i.e., when the data is spread across multiple classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def entropy(class_probabilities):\n",
    "    \"\"\"given a list of class probabilities, compute the entropy\"\"\"\n",
    "    import math\n",
    "    return sum(-p * math.log(p, 2) for p in class_probabilities if p) # ignore zero probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Entropy of Grade Distributions\n",
    "\n",
    "In project 3 we used k-means to automatically grade students. When I ran k-means with $k=5$ and `random_state=0`, I got the follow predictions:\n",
    "\n",
    "> **24 As, 29 Bs, 23 Cs, 13 Ds, 6 Fs**\n",
    "\n",
    "In contrast, the Duke professor gave\n",
    "\n",
    "> **45 As, 33 Bs, 11 Cs, 5 Ds, 1 F**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Entropy of the K-Means Grades distribution is 2.16377176406274\n",
      "The Entropy of the Duke Professor's distribution is 1.693415781456091\n"
     ]
    }
   ],
   "source": [
    "kmeansGrades = [24, 29, 23, 13, 6]\n",
    "DukeGrades = [45, 33, 11, 5, 1]\n",
    "\n",
    "kDistro = [i/95 for i in kmeansGrades]\n",
    "dDistro = [i/95 for i in DukeGrades]\n",
    "print(\"The Entropy of the K-Means Grades distribution is \"+ str(entropy(kDistro)))\n",
    "print(\"The Entropy of the Duke Professor's distribution is \" + str(entropy(dDistro)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Measuring Entropy\n",
    "\n",
    "Correspondingly, we’d like some notion of the entropy that results from partitioning a set of data in a certain way. We want a partition to have low entropy if it splits the data into subsets that themselves have low entropy (i.e., are highly certain), and high entropy if it contains subsets that (are large and) have high entropy (i.e., are highly\n",
    "uncertain). This leads to the notion of **Information Gain**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Information Gain\n",
    "\n",
    "If we partition our data $S$ into subsets $S_1, \\cdots, S_m$ containing proportions $q_1, \\cdots, q_m$\n",
    "of the data, then we compute the *entropy of the partition* as a weighted sum:\n",
    "$$E(S_1, \\cdots, S_m) = q_1E(S_1)+ \\cdots + q_mE(S_m) $$\n",
    "\n",
    "Where each $E(S_i)$ is the entropy of the empirical probability distribution of class labels within in the subset $S_i$. Consequently, we can think of \n",
    "- $E(S_1,\\cdots,S_m)$ is the *average entropy* across the partition. \n",
    "- Notice that the number $m$ of sets in the partition is not necessarily equal to the number $n$ of class labels.\n",
    "\n",
    "Then the ***information gained*** would be calculated as the entropy of the data $S$ before the partition minus the entropy of the partition :\n",
    "\n",
    "$$E = E(S) - E(S_1, \\cdots, S_m) $$\n",
    "\n",
    "**Question: What partition maximizes the information gained?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The K-Means Project Reconsidered\n",
    "\n",
    "Let's analyze the grading provided by K-means more closely:\n",
    "\n",
    "- The k-means A (call it a k-A) cluster consisted of 24 Duke As\n",
    "- The k-B cluster consisted of 21 Duke As and 8 Duke Bs\n",
    "- The k-C cluster consisted of 23 Duke Bs\n",
    "- The k-D cluster consisted of 2 Duke Bs and 11 Duke Cs\n",
    "- The k-F cluster consisted of 5 Duke Ds and 1 Duke F\n",
    "\n",
    "Let's analyze the entropy of this partition, treating the Duke grades as labels and the K-means labels as clusters in a partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Entropy of the k-Means partition is 0.38520932674799657\n",
      "Information Gained is 1.7785624373147433\n",
      "For comparison, the overall purity of k-means partition ends up being .884\n"
     ]
    }
   ],
   "source": [
    "kA = [24/24]\n",
    "kB = [21/29, 8/29]\n",
    "kC = [23/23]\n",
    "kD = [11/13, 2/13]\n",
    "kF = [5/6, 1/6]\n",
    "\n",
    "partitionEntropy = (24/95)*entropy(kA)+ (29/95)*entropy(kB)+(23/95)*entropy(kC)+(13/95)*entropy(kD) + (6/95)*entropy(kF)\n",
    "print(\"The Entropy of the k-Means partition is \" + str(partitionEntropy))\n",
    "diff=entropy(kDistro)-partitionEntropy\n",
    "print(\"Information Gained is \"+ str(diff))\n",
    "print(\"For comparison, the overall purity of k-means partition ends up being .884\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Digression on Information Theory and Entropy\n",
    "\n",
    "![Claude Shannon](Shannon_and_mouse.png)\n",
    "\n",
    "The definition of entropy using a sum (or integral) of $p_i\\log_2 p_i$ is due to [Claude Shannon (1916-2001)](https://en.wikipedia.org/wiki/Claude_Shannon) who apparently decided to call his quantity *entropy* after [John von Neumann](https://en.wikipedia.org/wiki/John_von_Neumann) encouraged him to use that word because \"nobody really knows that entropy really is.\" (Bishop, p.55). \n",
    "\n",
    "Shannon had a storied life after going to college at 16, inventing digital circuit theory in his master's thesis when he was 21, promoting that to a PhD thesis when he was 24, and then worked at Bell labs where he defined information-theoretic entropy and coined the word **bit**, which is the basis 0-or-1 unit of information, when he was 32.\n",
    "\n",
    "As you can read in [Bishop's book](https://www.springer.com/gp/book/9780387310732) on pages 48-58, you can think of **information** as the degree of surprise of learning the value of a random variable $X$. Moreover, if you observe two independent events $X=x$ and $X=y$ then you'd want the information associated to the this pair\n",
    "$$h(x,y)=h(x) + h(y)$$\n",
    "But since for independent events\n",
    "$$p(x,y)=p(x)p(y)$$\n",
    "then we can surmise that whatever information is, ***it has to turn products of events to sums, and hence it should be proportional to the logarithm:***\n",
    "$$h(x)=-\\log p(x)$$\n",
    "The negative sign makes sure this number is positive, since logarithms of numbers less than 1 is negative or undefined. Moreover, in order to make the limit of this function well defined as we move towards $p=0$ multiplying by $p$ makes the term $$p(x)\\log p(x)$$ well defined.\n",
    "\n",
    "Alternative notation to the function $E$ we've used above is the notation $H$. You will sometimes see information entropy defined for a discrete random variable $X$ and probability mass function $p(x)$, which is the probability of observing the outcome $X=x$, as follows:\n",
    "$$H[X] = -\\sum_{x} p(x) \\log_2 p(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Example\n",
    "\n",
    "Consider that you are provided with interviewee data, consisting of (per your specification) pairs (input, label), where each input is a dict of candidate attributes, and each label is either True (the candidate interviewed well) or False (the candidate interviewed poorly). In particular, you are provided with each candidate’s level, her preferred language, whether she is active on Twitter, and whether she has a PhD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def class_probabilities(labels):\n",
    "    import collections as c\n",
    "    total_count = len(labels)\n",
    "    return [count / total_count for count in c.Counter(labels).values()]\n",
    "\n",
    "def data_entropy(labeled_data):\n",
    "    labels = [label for _, label in labeled_data]\n",
    "    probabilities = class_probabilities(labels)\n",
    "    return entropy(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def partition_entropy(subsets):\n",
    "    \"\"\"find the entropy from this partition of data into subsets\"\"\"\n",
    "    total_count = sum(len(subset) for subset in subsets)\n",
    "    return sum( data_entropy(subset) * len(subset) / total_count for subset in subsets )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "inputs = [({'level':'Senior', 'lang':'Java', 'tweets':'no', 'phd':'no'}, False),\n",
    " ({'level':'Senior', 'lang':'Java', 'tweets':'no', 'phd':'yes'}, False),\n",
    " ({'level':'Mid', 'lang':'Python', 'tweets':'no', 'phd':'no'}, True),\n",
    " ({'level':'Junior', 'lang':'Python', 'tweets':'no', 'phd':'no'}, True),\n",
    " ({'level':'Junior', 'lang':'R', 'tweets':'yes', 'phd':'no'}, True),\n",
    " ({'level':'Junior', 'lang':'R', 'tweets':'yes', 'phd':'yes'}, False),\n",
    " ({'level':'Mid', 'lang':'R', 'tweets':'yes', 'phd':'yes'}, True),\n",
    " ({'level':'Senior', 'lang':'Python', 'tweets':'no', 'phd':'no'}, False),\n",
    " ({'level':'Senior', 'lang':'R', 'tweets':'yes', 'phd':'no'}, True),\n",
    " ({'level':'Junior', 'lang':'Python', 'tweets':'yes', 'phd':'no'}, True),\n",
    " ({'level':'Senior', 'lang':'Python', 'tweets':'yes', 'phd':'yes'}, True),\n",
    " ({'level':'Mid', 'lang':'Python', 'tweets':'no', 'phd':'yes'}, True),\n",
    " ({'level':'Mid', 'lang':'Java', 'tweets':'yes', 'phd':'no'}, True),\n",
    " ({'level':'Junior', 'lang':'Python', 'tweets':'no', 'phd':'yes'}, False)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ID3 Algorithm\n",
    "\n",
    "Let’s say we’re given some labeled data, and a list of attributes to consider branching on. Then the ID3(Iterative Dichotomiser 3) tree algorithm is:\n",
    "* If the data all have the same label, then create a leaf node that predicts that label and then stop.\n",
    "\n",
    "* If the list of attributes is empty (i.e., there are no more possible questions to ask), then create a leaf node that predicts the most common label and then stop.\n",
    "\n",
    "* Otherwise, try partitioning the data by each of the attributes\n",
    "\n",
    "* Choose the partition with the lowest partition entropy\n",
    "\n",
    "* Add a decision node based on the chosen attribute\n",
    "\n",
    "* Recur on each partitioned subset using the remaining attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def partition_by(inputs, attribute):\n",
    "    \"\"\"each input is a pair (attribute_dict, label). Returns a dict : attribute_value -> inputs\"\"\"\n",
    "    from collections import defaultdict\n",
    "    groups = defaultdict(list)\n",
    "    for input in inputs:\n",
    "        key = input[0][attribute] # get the value of the specified attribute\n",
    "        groups[key].append(input) # then add this input to the correct list\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0.6935361388961919\n",
      "lang 0.8601317128547441\n",
      "tweets 0.7884504573082896\n",
      "phd 0.8921589282623617\n"
     ]
    }
   ],
   "source": [
    "def partition_entropy_by(inputs, attribute):\n",
    "    \"\"\"computes the entropy corresponding to the given partition\"\"\"\n",
    "    partitions = partition_by(inputs, attribute)\n",
    "    return partition_entropy(partitions.values())\n",
    "for key in ['level','lang','tweets','phd']:\n",
    "    print(key, partition_entropy_by(inputs, key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example:\n",
    "The *lowest* entropy comes from splitting on level, so we’ll need to make a subtree for each possible level value. Every Mid candidate is labeled True, which means that the Mid subtree is simply a leaf node predicting True. For Senior candidates, we have a mix of Trues and Falses, so we need to split again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang 0.4\n",
      "tweets 0.0\n",
      "phd 0.9509775004326938\n"
     ]
    }
   ],
   "source": [
    "senior_inputs = [(input, label) for input, label in inputs if input[\"level\"] == \"Senior\"]\n",
    "for key in ['lang', 'tweets', 'phd']:\n",
    "    print(key, partition_entropy_by(senior_inputs, key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "This shows us that our next split should be on tweets, which results in a zeroentropy partition. For these Senior-level candidates, “yes” tweets always result in True while “no” tweets always result in False. Finally, if we do the same thing for the Junior candidates, we end up splitting on phd, after which we find that no PhD always results in True and PhD always results in False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decision Tree for Hiring\n",
    "\n",
    "<center>\n",
    "    <img src=\"HireTree.png\" alt=\"Hiring Decisions\" width=\"500\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overfitting\n",
    "\n",
    "Given how closely decision trees can fit themselves to their training data, they tend to go too deep into the tree and *overfit* the data. Consider the two trees given by the first and second half of our original input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def partition_by_2(inputs, attribute):\n",
    "    from collections import defaultdict\n",
    "    groups = defaultdict(list)\n",
    "    for input in inputs[:len(inputs)//2]:\n",
    "        key = input[0][attribute] # get the value of the specified attribute\n",
    "        groups[key].append(input) # then add this input to the correct list\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0.3935553574519241\n",
      "lang 0.3935553574519241\n",
      "tweets 0.9649839288804956\n",
      "phd 0.8571428571428572\n"
     ]
    }
   ],
   "source": [
    "def partition_entropy_by_2(inputs, attribute):\n",
    "    partitions = partition_by_2(inputs, attribute)\n",
    "    return partition_entropy(partitions.values())\n",
    "for key in ['level','lang','tweets','phd']:\n",
    "    print(key, partition_entropy_by_2(inputs, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def partition_by_3(inputs, attribute):\n",
    "    from collections import defaultdict\n",
    "    groups = defaultdict(list)\n",
    "    for input in inputs[len(inputs)//2:]:\n",
    "        key = input[0][attribute] # get the value of the specified attribute\n",
    "        groups[key].append(input) # then add this input to the correct list\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0.6792696431662097\n",
      "lang 0.6935361388961919\n",
      "tweets 0.3935553574519241\n",
      "phd 0.8571428571428572\n"
     ]
    }
   ],
   "source": [
    "def partition_entropy_by_3(inputs, attribute):\n",
    "    \"\"\"computes the entropy corresponding to the given partition\"\"\"\n",
    "    partitions = partition_by_3(inputs, attribute)\n",
    "    return partition_entropy(partitions.values())\n",
    "for key in ['level','lang','tweets','phd']:\n",
    "    print(key, partition_entropy_by_3(inputs, key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overfitting\n",
    "\n",
    "Even though there is some overlap, these two trees give two different classifications, i.e., level and language vs tweets. Therefore, some people will use information from several trees in order to further improve our results. One way of doing this is a technique called **random forests**, in which we build multiple decision trees and let them vote on how to classify inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Resources\n",
    "\n",
    "* https://bricaud.github.io/personal-blog/entropy-in-decision-trees/\n",
    "* https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8\n",
    "* http://backspaces.net/temp/Ebooks/DataScienceFromScratch/Data_Science_from_Scratch.pdf"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
