{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# AMAT 502: Modern Computing for Mathematicians\n",
    "## Lecture 12 - Hypothesis Testing and the Central Limit Theorem\n",
    "### University at Albany SUNY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topics for Today\n",
    "- Basic Hypothesis Testing\n",
    "    - Two-Tail Test\n",
    "    - Upper and Lower Tails\n",
    "- Math Behind Hypothesis Testing\n",
    "    - Review of Mean and Variance\n",
    "    - Bernoulii RVs\n",
    "    - Binomial RVs\n",
    "    - The Normal Approximation to the Binomial\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hypothesis Testing\n",
    "\n",
    "Hypothesis testing determines whether an observation (or several observations) is **statistically significant**, in other words the observation is unusual enough that \n",
    "- it calls into question our basic beliefs, and\n",
    "- we are unwilling to attribute our observation to noise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Determining if a Coin is Fair\n",
    "\n",
    "Suppose I hand you a coin and you flip it 100 times and get 62 heads and 38 tails. \n",
    "\n",
    "Two questions:\n",
    "1. Would you believe that the coin is fair or that it's biased towards heads?\n",
    "2. How would you even compute the probability of observing such an event?\n",
    "\n",
    "![Coin Toss](coin-toss.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Frequentist Probability\n",
    "\n",
    "The traditional perspective on probability, called the **frequentist view**, is that probability is the percent of time an event occurs. Meaning if I flip a coin a bunch of times I can estimate the probability of heads by counting the number of times I get heads divided by the total number of flips.\n",
    "\n",
    "However in the example above we've done a single experiment: we flipped a coin 100 times and got 62 heads. I would feel more confident in making predictions if I could run this 100-flip experiment, say, 50 more times just to get a sense of what the numbers are like among those 50 experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Probability\n",
    "\n",
    "Although still quite old, the **Bayesian** approach to probability has become more popular with the increased prevalence of computers. It is also the perspective on probability most favored by modern data scientists.\n",
    "\n",
    "The Bayesian view says that **probability is how much you believe something to be true.**\n",
    "\n",
    "This is especially helpful when you can't run multiple experiements. For example, consider the question:\n",
    "\n",
    "    What's the probability of life on Mars?\n",
    "\n",
    "You can't run experiments where we simulate the creation of the universe multiple times and count the number of times life arises on Mars. **The frequentist approach to probability is useless in this example.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beliefs about Coins\n",
    "\n",
    "It seems reasonable to believe that any coin someone gives you that has two distinct sides (heads and tails) the chance of getting either one is roughly equal.\n",
    "\n",
    "After all a coin is not like jelly toast, [where the jelly side lands face down about 2/3rds of the time.](https://www.sciencefocus.com/science/why-does-toast-often-land-butter-side-down/)\n",
    "\n",
    "![Jelly Toast](jelly-toast.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Null Hypothesis\n",
    "\n",
    "In the absence of new evidence, what we believe to be true is called the **null hypothesis**, which we write as $H_0$.\n",
    "\n",
    "*N.B. \"Null\" is another way of saying \"zero\"*\n",
    "\n",
    "Here are some examples of null hypotheses:\n",
    "1. $H_0$: The probability of getting a heads is 1/2.\n",
    "2. $H_0$: An aspirin helps 60% of all headaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Alternative Hypothesis\n",
    "\n",
    "Of course, we are always confronted with new data where we might want to consider **alternative hypotheses,** which we write as $H_a$ or $H_1$.\n",
    "\n",
    "1. $H_a$: With the coin we flipped above, the  probability of getting a heads is *not* equal to 1/2.\n",
    "2. $H_a$: A new aspirin, called *AspirinXTREME* is more effective for treating headaches than aspirin.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple Hypothesis Testing (Two-Tailed Test)\n",
    "\n",
    "In our coin-flipping example, the alternative hypothesis $H_a$ asserts that the probability $p$ of getting a heads is not equal to $p_0=1/2$, which is the null hypothesis. This means that $p$ could be above or below $p_0$.\n",
    "\n",
    "In this case, we use a **two-tailed test**. [Our decision procedure goes as follows:](https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_hypothesistest-means-proportions/bs704_hypothesistest-means-proportions3.html)\n",
    "\n",
    "1. **Select the significance level $\\alpha$.** The most common value for $\\alpha$ is .05.\n",
    "2. **Compute the $Z$-score of our observation.** This measures the number of standard deviations from the mean where our observation $\\bar{X}$ lies, under the assumption that the null hypothesis is true. The quantity $\\bar{X}$ is the empirical average that we observed in our sample, but $p_0$ and $\\sigma_0$ are the mean and standard deviation of the null hypothesis model. We compute the Z-score via the formula\n",
    "$$ Z= \\frac{\\bar{X} - p_0}{\\sigma_0/\\sqrt{n}}$$\n",
    "3. **Compute the probability of getting something at least as extreme as the one you observed, i.e.** For a two-tailed test the **p-value** of our observation is $$p=P(x< -|Z|) + P(x> |Z|)=2P(x< -|Z|)$$\n",
    "4. **If our p-value $p$ is less than $\\alpha$ then we reject the null hypothesis in favor of the alternative hypothesis.**\n",
    "\n",
    "![Two Tailed Test](two-tail-test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Our Coin Example\n",
    "\n",
    "Our null hypothesis about a coin is that $p_0=1/2$ and $\\sigma_0=\\sqrt{1/4}=1/2$.\n",
    "\n",
    "Our alternative hypothesis is that $p\\neq p_0$, so we use a two-tail test.\n",
    "\n",
    "When we flipped the coin 100 times we got an empirical average (also called **sample mean**) $$\\bar{X}=62/100=.62$$\n",
    "\n",
    "This gives us a Z-score of $$Z= \\frac{.62-.5}{.5/\\sqrt{100}}=\\frac{.12}{.5/10}=2.4$$\n",
    "\n",
    "By looking up in a [Z-score table](http://www.z-table.com/), we conclude that our p-value is $$P(x< -2.4) + P(x> 2.4) = 2(.0082)=.0164$$\n",
    "\n",
    "Since $$p = .0164 < .05 =\\alpha$$ we reject the null hypothesis that the coin is fair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Upper Tail Test\n",
    "\n",
    "If our alternative hypothesis $H_a$ asserts that $p>p_0$, e.g. **heads is more likely than .5** then we change how we compute our p-value in step 3 to $$p= P(x > Z)$$\n",
    "\n",
    "![Upper Tail Test](upper-tail-test.png)\n",
    "\n",
    "### Our Coin Example\n",
    "\n",
    "If we modified our alternative hypothesis $H_a$ to be $p>p_0$ and kept $\\alpha=.05$ then we would have required a Z-score of greater than 1.645 in order to reject the null hypothesis.\n",
    "\n",
    "### Changing $\\alpha$\n",
    "If made our significance level $\\alpha=.005$. In our coin example we got $Z=2.4 < 2.576$, so **we would NOT reject the null hypothesis**\n",
    "\n",
    "To calculate how many heads we'd need to reject the null hypothesis in favor of this alternative hypothesis we can set\n",
    "\n",
    "$$2.576=\\frac{\\bar{X} - .5}{.5/\\sqrt{100}} \\Rightarrow \\bar{X}= .5 + 2.576(.05) = .6288 $$\n",
    "In other words we'd need 63 or more heads to reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lower Tail Test\n",
    "\n",
    "If our alternative hypothesis $H_a$ asserts that $p<p_0$, e.g. **heads is less likely than .5** then we change how we compute our p-value in step 3 to $$p= P(x < Z)$$\n",
    "\n",
    "![Lower Tail Test](lower-tail-test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Math Behind Hypothesis Testing\n",
    "\n",
    "In doing the above computations, specifically when we compute the Z-score and the corresponding p-value, we're treating a discrete probability problem as if it were modeled using normal distribution.\n",
    "\n",
    "    Why can we do this?\n",
    "\n",
    "To see why we can model coin flips with a normal distribution, we need to review some discrete probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary Statistics: Mean and Standard Deviation\n",
    "\n",
    "A discrete random variable $X$ is determined by its **probability mass function (PMF)** $p_X(x)$. The following are some common numerical quantities that we use to summarize the PMF of $X$.\n",
    "\n",
    "The **mean** or **expectation** of a discrete RV is $$\\mu_X:= E(X) := \\sum_x x\\cdot p_X(x)$$\n",
    "\n",
    "The **variance** of a discrete random variable is $$\\sigma^2_X := V(X) := E((X-\\mu_X)^2) = E(X^2) - \\mu_X^2 = (\\sum_x x^2\\cdot p_X(x)) - \\mu_X^2 $$\n",
    "\n",
    "The **standard deviation** is the positive square root of the variance, i.e. $$D(X) = \\sqrt{V(X)} = \\sigma_X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bernoulli Random Variables\n",
    "\n",
    "A **Bernoulli Random Variable** (RV) $X$ is a variable that takes the value 1 with probability $p$ and 0 with probability $q=1-p$.\n",
    "\n",
    "Here is the probability mass function (PMF) of a Bernoulli RV $X$:\n",
    "\n",
    "| X |  0 | 1  |\n",
    "|---|---|---|\n",
    "|p(x) | q  | p  |\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "We can interpret $X=1$ as a \"success\" when performing a trial with only two outcomes.\n",
    "\n",
    "We can model a coin flip by saying $X=1$ corresponds to a heads and $X=0$ corresponds to a tails.\n",
    "\n",
    "### Mean and Standard Deviation\n",
    "\n",
    "$$E(X) = \\sum_x x\\cdot p_X(x) = 0\\cdot q + 1 \\cdot p =p$$\n",
    "$$V(X) = E(X^2) - E(X)^2 = (0^2 q + 1^2 p) - p^2 = p-p^2 = p(1-p)=p q$$\n",
    "\n",
    "#### Coin Example\n",
    "\n",
    "For a fair coin we used the fact that $\\sigma_0 = \\sqrt{p_0(1-p_0)}=\\sqrt{1/4}=1/2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Binomial Random Variables\n",
    "\n",
    "The sum of $n$ IID Bernoulli random variables (RVs) $X_1,\\ldots,X_n$ is a **Binomial Random Variable**, i.e. $$S_n:=X_1+\\cdots+X_n$$\n",
    "\n",
    "Recall that IID means **independent identically distributed**, i.e they have the same mean and standard deviation.\n",
    "\n",
    "\n",
    "The PMF for $S_n$ $\\text{Binom}(n,p)$ says $$P(S_n=k) =\\binom{n}{k}p^kq^{n-k}$$ \n",
    "\n",
    "### Interpretation\n",
    "We can interpret $S_n$ as counting the number of \"successes\" out of $n$ trials.\n",
    "\n",
    "**When we flip a coin $n$ times, the number of heads follows a binomial distribution.** \n",
    "\n",
    "$P(S_n=k)=p_{S_n}(k)$ is the probability of getting $k$ heads out of $n$ flips.\n",
    "\n",
    "### Mean and Standard Deviation\n",
    "By linearity of expectation and the IID property\n",
    "$$E(S_n) = E(X_1 + \\cdots + X_n) = E(X_1) + \\cdots + E(X_n) = np$$\n",
    "\n",
    "Variance is linear for independent variables, so\n",
    "$$V(S_n) = npq \\Rightarrow D(S_n) = \\sqrt{npq}$$\n",
    "\n",
    "#### Coin Example\n",
    "If we count the number of heads out of 100 flips, this follows a binomial RV $S_{100}$ with $p=1/2$ (assuming the null hypothesis is true). \n",
    "\n",
    "This means that $$D(S_{100})=\\sqrt{100\\cdot \\frac{1}{2}\\cdot \\frac{1}{2}} = \\sqrt{25}=5$$\n",
    "is one standard deviation for this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Central Limit Theorem for Binomial RVs\n",
    "\n",
    "The **Central Limit Theorem (CLT)** is a convergence theorem for IID sums of random variables of any type. \n",
    "\n",
    "For binomial random variables this says that for fixed $p$ and large $n$ the binomial distribution \"looks like\" a normal distribution after appropriate scaling. \n",
    "\n",
    "For our purposes, we can think of the following as the CLT.\n",
    "\n",
    "### Normal Approximation to Binomial\n",
    "\n",
    "**Theorem:** Let $S_n\\sim \\text{Binom}(n,p)$. If $np(1-p)>10$ then for integers $a<b$ we that the probability that $S_n\\in[a,b]$ can be calculated using the area under the curve of a unit normal PDF.\n",
    "\n",
    "To this end let\n",
    "$$\\Phi(s):=\\int_{-\\infty}^s \\frac{e^{-x^2/2}}{\\sqrt{2\\pi}} dx$$\n",
    "be the cumulative distribution function (CDF) of the unit normal distribution.\n",
    "\n",
    "The normal approximation says that $$P(a\\leq S_n \\leq b) = P(a^*\\leq \\frac{S_n - np}{\\sqrt{np(1-p)}} \\leq b^*) \\approx \\Phi(b^*) - \\Phi(a^*)$$\n",
    "where\n",
    "$$a^*=\\frac{a -np}{\\sqrt{np(1-p)}} \\qquad \\text{and} \\qquad b^* = \\frac{b -np}{\\sqrt{np(1-p)}}$$\n",
    "are the **z-transforms** of $a$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Z-transform\n",
    "\n",
    "Any random variable $X$ can we converted to **standard units**\n",
    "$$\n",
    "Z = \\frac{X - \\mu_X}{\\sigma_X}\n",
    "$$\n",
    "\n",
    "$Z$ measures how far $X$ is from the mean, in units of the SD. In other words $Z$ measures how many SDs above average the value of $X$ is.\n",
    "\n",
    "By linearity of expectation and properties of variance\n",
    "$$\n",
    "E(Z) = 0 ~~~ \\text{and} ~~~ D(Z) = 1\n",
    "$$\n",
    "no matter what the distribution of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Application to Hypothesis Testing\n",
    "\n",
    "At the start of the lecture we considered a quantity different from $S_n$, we used the **average RV** $$\\bar{X} := A_n =\\frac{S_n}{n} = \\frac{X_1 + \\cdots + X_n}{n}$$ associated to the null hypothesis where \n",
    "$$E(X_i)=p_0 \\qquad \\text{and} \\qquad D(X_i)=\\sigma_0=\\sqrt{p_0(1-p_0)}$$\n",
    "\n",
    "If we inspect the expression $$\\frac{S_n - np_0}{\\sqrt{np_0(1-p_0)}} = \\frac{S_n - np_0}{\\sigma_0 \\sqrt{n}}$$\n",
    "we can factor out an $n$ from the top to get\n",
    "$$\\frac{n(\\frac{S_n}{n} - p_0)}{\\sigma_0 \\sqrt{n}} = \\frac{\\sqrt{n}(\\frac{S_n}{n} - p_0)}{\\sigma_0} =\\frac{(\\frac{S_n}{n} - p_0)}{\\sigma_0/\\sqrt{n}}$$\n",
    "which after observing $S_n/n=\\bar{X}$ is exactly the expression we used to determine the Z-value of our experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More Reading\n",
    "\n",
    "- http://20bits.com/article/hypothesis-testing-the-basics\n",
    "- [Grinstead and Snell](https://math.dartmouth.edu/~prob/prob/prob.pdf)\n",
    "- Berkeley's Data Science Sequence:\n",
    "    - [Data8](http://data8.org/)\n",
    "    - [Textbook for Data8](https://www.inferentialthinking.com/chapters/intro)\n",
    "    - [Berkeley's Prob 140 Course](http://prob140.org/textbook/content/README.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
